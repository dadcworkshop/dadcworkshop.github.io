+++
title = "Program"
template = "page.html"
+++

# Invited Speakers and Program

## <span style="color:#267CB9"> Invited Speakers </span>

<div class="invited-speakers">

### [Anna Rogers, University of Copenhagen](https://annargrs.github.io)
#### What kinds of questions have we been asking? A taxonomy for QA/RC benchmarks

{{ resize_image(path="static/img/anna-rogers.jpeg", width=100, height=100, op="scale") }}

This talk provides an overview of the current landscape of resources for Question Answering and Reading comprehension, highlighting the current lacunae for future work. I will also present a new taxonomy of "skills" targeted by QA/RC datasets and discuss various ways in which questions may be unanswerable.

<hr />


### [Sam Bowman, Assistant Professor, New York University & Visiting Researcher (Sabbatical), Anthropic](https://cims.nyu.edu/~sbowman/)
#### Why Adversarially-Collected Test Sets Don’t Work as Benchmarks

{{ resize_image(path="static/img/sambowman.jpeg", width=100, height=100, op="fill") }}

Dynamic and/or adversarial data collection can be quite useful as a way of collecting training data for machine-learning models, identifying the conditions under which these models fail, and conducting online head-to-head comparisons between models. However, it is essentially impossible to use these practices to build usable static benchmark datasets for use in evaluating or comparing future new models. I defend this point using a mix of conceptual and empirical points, focusing on the claims (i) that adversarial data collection can skew the distribution of phenomena such as to make it unrepresentative of the intended task, and (ii) that adversarial data collection can arbitrarily shift the rankings of models on its resulting test sets to disfavor systems that are qualitatively similar to the current state of the art.

<hr />


### [Jordan Boyd-Graber, Associate Professor, University of Maryland at College Park](http://users.umiacs.umd.edu/~jbg/)
#### Incentives for Experts to Create Adversarial QA and Fact-Checking Examples

{{ resize_image(path="static/img/jbg.png", width=100, height=100, op="scale") }}

I'll discuss two examples of our work putting experienced writers in
front of a retrieval-driven adversarial authoring system: question
writing and fact-checking.  For question answering, we develop a
retrieval-based adversarial authoring platform and create incentives
to get people to use our system in the first place, write
interesting questions humans can answer, and challenge a QA system.
While the best humans lose to computer QA systems on normal questions,
computers struggle to answer our adversarial questions.  We then turn
to fact checking, creating a new game (Fool Me Twice) to solicit
difficult-to-verify claims---that can be either true or false---and to
test how difficult the claims are both for humans and computers.  We
argue that the focus on retrieval is important for knowledge-based
adversarial examples because it highlights diverse information,
prevents frustration in authors, and takes advantage of users'
expertise.

<hr />


### [Lora Aroyo, Research Scientist, Google](https://lora-aroyo.org)
#### Data Excellence: Better Data for Better AI

{{ resize_image(path="static/img/lora-aroyo.jpg", width=100, height=100, op="fill") }}

The efficacy of machine learning (ML) models depends on both algorithms and data. Training data defines what we want our models to learn, and testing data provides the means by which their empirical progress is measured. Benchmark datasets define the entire world within which models exist and operate, yet research continues to focus on critiquing and improving the algorithmic aspect of the models rather than critiquing and improving the data with which our models operate. If “data is the new oil,” we are still missing work on the refineries by which the data itself could be optimized for more effective use. In this talk, I will discuss data excellence and lessons learned from software engineering to achieve the scare and rigor in assessing data quality.

<hr />


### [Sherry Tongshuang Wu, Assistant Professor, Carnegie Mellon University (CMU HCII)](https://homes.cs.washington.edu/~wtshuang/)
#### Model-in-the-loop Data Collection: What Roles does the Model Play?

{{ resize_image(path="static/img/sherry-wu.jpeg", width=100, height=100, op="fill") }}

Assistive models have been shown useful for supporting humans in creating challenging datasets, but how exactly do they help? In this talk, I will discuss different roles of assistive models in counterfactual data collection (i.e., perturbing existing text inputs to gain insight into task model decision boundaries), and the characteristics associated with these roles. I will use three examples (CheckList, Polyjuice, Tailor) to demonstrate how our objectives shift when we perturb texts for evaluation, explanation, and improvement, and how that change the corresponding assistive models from enhancing human goals (requiring model controllability) to competing with human bias (requiring careful data reranking). I will conclude by exploring additional roles that these models can play to become more effective.

<hr />

</div>


## <span style="color:#267CB9">Program</span>

Date: 14<sup>th</sup> July, 2022<br />

<span class="time">09:00 – 09:10</span> Opening remarks

<span class="time">09:10 – 09:25</span> Collaborative Progress: [MLCommons](https://mlcommons.org/) Introduction

<span class="time">09:25 – 10:00</span> Invited Talk 1: Anna Rogers

<span class="time">10:00 – 10:35</span> Invited Talk 2: Jordan Boyd-Graber

<span class="time">10:35 – 10:50</span> Coffee Break

<span class="time">10:50 – 11:10</span> Best Paper Talk: Margaret Li and Julian Michael

<span class="time">11:10 – 11:45</span> Invited Talk 3: Sam Bowman

<span class="time">11:45 – 12:20</span> Invited Talk 4: Lora Aroyo

<span class="time">12:20 – 13:20</span> Lunch

<span class="time">13:20 – 13:55</span> Invited Talk 5: Sherry Tongshuang Wu

<span class="time">13:55 – 14:55</span> Panel: The Future of Data Collection. Moderated by Adina Williams

<span class="time">14:55 – 15:10</span> Coffee Break

<span class="time">15:10 – 15:20</span> Shared Task Introduction

<span class="time">15:20 – 15:40</span> Shared Task Winners' Presentations

<span class="time">15:40 – 16:50</span> Poster Session

<span class="time">16:50 – 17:00</span> Closing Remarks

<hr />